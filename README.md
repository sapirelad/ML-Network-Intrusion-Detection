# ML-Based Network Intrusion Detection  
### A Comparative Study on CICIDS2017 and UNSW-NB15

## Project Overview
This project investigates the application of machine learning techniques for **network intrusion detection (IDS)** using flow-level network traffic data.  
The main goal is to distinguish between **benign traffic and malicious attacks**, while examining how well models generalize across different datasets.

Two widely used IDS benchmarks are analyzed:
- **CICIDS2017** – modern, realistic traffic with severe class imbalance and known data quality issues.
- **UNSW-NB15** – a more balanced dataset with a different feature design and attack distribution.

Beyond standard supervised learning, the project explores **graph-based and semi-supervised methods**, as well as **cross-dataset generalization**, which is critical for real-world IDS deployment.

---

## Datasets

### CICIDS2017
- Generated by the Canadian Institute for Cybersecurity.
- Contains multiple attack types captured across different days and traffic scenarios.
- Known challenges:
  - Extreme class imbalance
  - Infinite and missing values
  - Dataset-specific artifacts (e.g., time-based patterns)

### UNSW-NB15
- Generated at the Australian Centre for Cyber Security.
- Includes synthetic normal traffic and modern attack behaviors.
- More balanced label distribution and cleaner numeric features.

---

## Methodology

### 1. Exploratory Data Analysis (EDA)
- Class distribution analysis (attack vs. benign)
- Feature value inspection (skewness, outliers, Inf/NaN values)
- Per-file and per-segment attack rate analysis (CICIDS2017)

### 2. Data Preprocessing
- Removal or treatment of infinite and missing values
- Feature scaling for numeric attributes
- Dataset-specific preprocessing pipelines
- Leakage-aware train/test splitting

### 3. Supervised Learning Baselines
The following models were trained and evaluated **within each dataset**:
- Dummy Classifier (most frequent)
- Logistic Regression (class-balanced)
- Random Forest

Evaluation metrics:
- PR-AUC (primary metric due to class imbalance)
- ROC-AUC
- Precision, Recall, F1-score (attack class)
- Confusion matrix statistics (TP, FP, TN, FN)

---

## Graph-Based and Semi-Supervised Learning

To reduce reliance on labeled data, graph-based methods were explored:

### Spectral Clustering
- Applied on reduced feature representations
- Evaluated using ARI and NMI against true labels
- Results indicate limited alignment with attack labels, highlighting overlapping traffic patterns

### Label Spreading (Semi-Supervised)
- Operates on a similarity graph constructed from feature space
- Uses a small labeled subset to propagate labels to unlabeled samples
- Demonstrates strong performance **within the same dataset**, especially under limited-label scenarios

---

## Cross-Dataset Evaluation (Key Contribution)

To test real-world generalization, **cross-dataset experiments** were conducted:

- Train on **CICIDS2017**, test on **UNSW-NB15**
- Train on **UNSW-NB15**, test on **CICIDS2017**
- Models were evaluated **without retraining**, using only shared feature families

### Findings:
- Strong performance in within-dataset evaluation does **not** transfer across datasets
- Significant drop in recall and PR-AUC in both directions
- Clear evidence of **dataset shift** and dataset-specific learning

This highlights a critical limitation of many IDS studies that evaluate models on a single dataset only.

---

## Threshold Tuning Under Dataset Shift

To simulate operational IDS deployment:
- Classification thresholds were adjusted during cross-dataset evaluation
- Lower thresholds increase attack recall but reduce precision

This demonstrates a real-world trade-off:
- **Security-focused settings** may favor recall
- **Operational environments** must balance false positives vs. missed attacks

---

## Key Findings

- IDS models can perform very well within a single dataset but fail to generalize.
- Dataset shift is a major challenge in network intrusion detection.
- Graph-based and semi-supervised methods are promising but not sufficient alone.
- Threshold calibration is crucial for deployment under distributional changes.
- Evaluating IDS models across datasets is essential for realistic assessment.

---

## Repository Contents

- `ML_For_Cyber-5.ipynb`  
  Complete Jupyter notebook containing:
  - Data loading and preprocessing  
  - EDA and visualization  
  - Supervised and semi-supervised models  
  - Cross-dataset experiments  
  - Evaluation and discussion  

---

## Notes
- Datasets are not included in the repository due to size constraints.
- All experiments were executed in Google Colab.
- Results and conclusions are fully reproducible using the notebook.

---

## Author
Sapir Elad 
